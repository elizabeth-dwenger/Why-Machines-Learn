{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea589961-8e34-4a70-866b-de08b82606be",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff8f90-590d-4cf8-98f2-0d511a624b96",
   "metadata": {},
   "source": [
    "## 1. Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem states:\n",
    "\n",
    "$$\n",
    "P(y | X) = \\frac{P(X | y) P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( P(y | X) \\) is the **posterior probability** (probability of class \\( y \\) given features \\( X \\)).\n",
    "- \\( P(X | y) \\) is the **likelihood** (probability of features \\( X \\) given class \\( y \\)).\n",
    "- \\( P(y) \\) is the **prior probability** of class \\( y \\).\n",
    "- \\( P(X) \\) is the **evidence** (total probability of \\( X \\) across all classes).\n",
    "\n",
    "Since \\( P(X) \\) is the same for all classes:\n",
    "\n",
    "$$\n",
    "P(y | X) \\propto P(X | y) P(y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Naïve Assumption (Feature Independence)\n",
    "\n",
    "If \\( X \\) has \\( n \\) features \\( x_1, x_2, ..., x_n \\), we assume **conditional independence**:\n",
    "\n",
    "$$\n",
    "P(X | y) = P(x_1 | y) \\cdot P(x_2 | y) \\cdot ... \\cdot P(x_n | y)\n",
    "$$\n",
    "\n",
    "Thus, Bayes’ Theorem simplifies to:\n",
    "\n",
    "$$\n",
    "P(y | X) \\propto P(y) \\prod_{i=1}^{n} P(x_i | y)\n",
    "$$\n",
    "\n",
    "This is why it’s called **\"Naïve\"**—it assumes that all features are **independent**, which may not always be true in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Classification Rule\n",
    "\n",
    "To classify a new instance \\( X \\), we compute the posterior for each class \\( y \\) and **choose the class with the highest probability**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{y} P(y) \\prod_{i=1}^{n} P(x_i | y)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( P(y) \\) is estimated as the **proportion of samples** in each class.\n",
    "- \\( P(x_i | y) \\) is estimated using **Maximum Likelihood Estimation (MLE)** or **Laplace Smoothing**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Log Probability for Numerical Stability\n",
    "\n",
    "Since probabilities can be very small, we take the **log** to avoid underflow:\n",
    "\n",
    "$$\n",
    "\\log P(y | X) = \\log P(y) + \\sum_{i=1}^{n} \\log P(x_i | y)\n",
    "$$\n",
    "\n",
    "This converts **multiplications into additions**, making computations more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f60494-a04d-476d-8ea5-2cc22e004f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e3bf10-4053-4b0d-a83a-38cf9a40126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}  # P(y) - Prior probabilities\n",
    "        self.feature_likelihoods = {}  # P(x|y) - Conditional probabilities\n",
    "        self.classes = None  # Unique class labels\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "\n",
    "        # Compute prior probabilities P(y)\n",
    "        self.class_priors = {c: np.mean(y == c) for c in self.classes}\n",
    "\n",
    "        # Compute likelihoods P(x|y) using Laplace smoothing\n",
    "        self.feature_likelihoods = {}\n",
    "\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]  # Subset of X where y == c\n",
    "            self.feature_likelihoods[c] = {}\n",
    "\n",
    "            for feature_idx in range(n_features):\n",
    "                # Count occurrences of each feature value\n",
    "                values, counts = np.unique(X_c[:, feature_idx], return_counts=True)\n",
    "                total_count = X_c.shape[0]\n",
    "\n",
    "                # Apply Laplace Smoothing: (count + 1) / (total + num_values)\n",
    "                likelihoods = {val: (count + 1) / (total_count + len(values)) for val, count in zip(values, counts)}\n",
    "                \n",
    "                # Store likelihoods, handling unseen values with smoothing\n",
    "                self.feature_likelihoods[c][feature_idx] = likelihoods\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            posteriors = {}\n",
    "\n",
    "            for c in self.classes:\n",
    "                # Start with the log prior P(y)\n",
    "                posterior = np.log(self.class_priors[c])\n",
    "\n",
    "                for feature_idx, feature_value in enumerate(x):\n",
    "                    # Add the log likelihood P(x|y), handling unseen values\n",
    "                    likelihoods = self.feature_likelihoods[c][feature_idx]\n",
    "                    posterior += np.log(likelihoods.get(feature_value, 1e-6))  # Small value for unseen cases\n",
    "\n",
    "                posteriors[c] = posterior\n",
    "\n",
    "            # Choose class with highest posterior probability\n",
    "            predictions.append(max(posteriors, key=posteriors.get))\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0efca9f-eb56-4b4b-9ad1-9211344d0ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example dataset (categorical features: Weather (0=sunny, 1=rainy), Temperature (0=cold, 1=hot))\n",
    "    X = np.array([\n",
    "        [0, 1],  # sunny, hot\n",
    "        [0, 0],  # sunny, cold\n",
    "        [1, 1],  # rainy, hot\n",
    "        [1, 0],  # rainy, cold\n",
    "    ])\n",
    "    y = np.array([1, 1, 0, 0])  # Play: yes=1, no=0\n",
    "\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(X, y)\n",
    "\n",
    "    X_test = np.array([\n",
    "        [0, 1],  # sunny, hot\n",
    "        [1, 0],  # rainy, cold\n",
    "    ])\n",
    "    predictions = nb.predict(X_test)\n",
    "    print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41af577-d29f-44f0-a322-238bd5f78036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
