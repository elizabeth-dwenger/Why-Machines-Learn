{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6516afcf-cc62-4015-ba3a-68b697f18906",
   "metadata": {},
   "source": [
    "# Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bc29e-edbf-4cc1-8cb3-162df5e39c1f",
   "metadata": {},
   "source": [
    "### **1. Layers and Weights**\n",
    "For a neural network with L layers, let:\n",
    "- $x \\in \\mathbb{R}^d$ be the input vector (of dimension d),\n",
    "- $W^{[l]} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ be the weight matrix for the l-th layer,\n",
    "- $b^{[l]} \\in \\mathbb{R}^{n_l}$ be the bias vector for the l-th layer,\n",
    "- $a^{[l]} \\in \\mathbb{R}^{n_l}$ be the activation vector for the l-th layer.\n",
    "\n",
    "The output of each layer is computed as:\n",
    "$$ z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]} $$\n",
    "where $a^{[l-1]}$ is the activation vector from the previous layer, and $z^{[l]}$ is the linear combination of inputs.\n",
    "\n",
    "### **2. Activation Functions**\n",
    "After computing the linear combination, the output is passed through an activation function $f(z)$:\n",
    "- **Sigmoid activation function**: \n",
    "  $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$  \n",
    "- **ReLU activation function**: \n",
    "  $$ \\text{ReLU}(z) = \\max(0, z) $$  \n",
    "\n",
    "For the output layer:\n",
    "- If the task is classification, the output could be the **softmax** or a simple **sigmoid** for binary classification.\n",
    "\n",
    "### **3. Forward Pass**\n",
    "The forward pass involves computing the activations at each layer until the final output:\n",
    "$$ a^{[l]} = f(z^{[l]}) $$\n",
    "\n",
    "### **4. Backpropagation**\n",
    "During training, **backpropagation** is used to adjust the weights. The gradients of the loss function L with respect to the weights are computed:\n",
    "$$ \\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial a^{[l]}} \\frac{\\partial a^{[l]}}{\\partial W^{[l]}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff04bebe-57d7-402a-9e10-94975d82c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activation_function='sigmoid'):\n",
    "        \"\"\"\n",
    "        layer_sizes: List containing the number of neurons in each layer.\n",
    "        activation_function: 'sigmoid' or 'relu'.\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = [np.random.randn(next_layer, current_layer) * 0.1 for current_layer, next_layer in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "        self.biases = [np.zeros((size, 1)) for size in layer_sizes[1:]]\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        self.a = [X]  # Store activations for each layer\n",
    "        self.z = []  # Store linear combinations for each layer\n",
    "        \n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            z = np.dot(self.weights[i], self.a[-1]) + self.biases[i]\n",
    "            self.z.append(z)\n",
    "            if self.activation_function == 'sigmoid':\n",
    "                a = self.sigmoid(z)\n",
    "            elif self.activation_function == 'relu':\n",
    "                a = self.relu(z)\n",
    "            self.a.append(a)\n",
    "        \n",
    "        return self.a[-1]\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"Backward pass to update weights and biases\"\"\"\n",
    "        m = X.shape[1]\n",
    "        self.dz = [self.a[-1] - y]  # Derivative of the cost function wrt to output\n",
    "        self.dw = [np.dot(self.dz[0], self.a[-2].T) / m]\n",
    "        self.db = [np.sum(self.dz[0], axis=1, keepdims=True) / m]\n",
    "\n",
    "        for l in range(len(self.layer_sizes) - 2, -1, -1):\n",
    "            if self.activation_function == 'sigmoid':\n",
    "                dz = self.dz[-1] * self.sigmoid_derivative(self.z[l])\n",
    "            elif self.activation_function == 'relu':\n",
    "                dz = self.dz[-1] * self.relu_derivative(self.z[l])\n",
    "            self.dz.append(dz)\n",
    "            \n",
    "            dw = np.dot(dz, self.a[l].T) / m\n",
    "            db = np.sum(dz, axis=1, keepdims=True) / m\n",
    "            self.dw.append(dw)\n",
    "            self.db.append(db)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights = [w - learning_rate * dw for w, dw in zip(self.weights, reversed(self.dw))]\n",
    "        self.biases = [b - learning_rate * db for b, db in zip(self.biases, reversed(self.db))]\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "        \"\"\"Train the MLP using forward and backward propagation\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            self.forward(X)\n",
    "            # Backward pass\n",
    "            self.backward(X, y, learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean(np.square(y - self.a[-1]))  # Mean squared error\n",
    "                print(f\"Epoch {epoch}: Loss = {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba946b6-25d7-4abf-bc33-4265232e91d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.2508933402077232\n",
      "Epoch 100: Loss = 0.2501496210621299\n",
      "Epoch 200: Loss = 0.2500240294205483\n",
      "Epoch 300: Loss = 0.2500032297799143\n",
      "Epoch 400: Loss = 0.24999982488751438\n",
      "Epoch 500: Loss = 0.2499992860049544\n",
      "Epoch 600: Loss = 0.24999921751241574\n",
      "Epoch 700: Loss = 0.24999922570234906\n",
      "Epoch 800: Loss = 0.24999924597992446\n",
      "Epoch 900: Loss = 0.2499992677667775\n",
      "Epoch 1000: Loss = 0.24999928934283983\n",
      "Epoch 1100: Loss = 0.2499993104400933\n",
      "Epoch 1200: Loss = 0.2499993310282968\n",
      "Epoch 1300: Loss = 0.24999935111583474\n",
      "Epoch 1400: Loss = 0.24999937071700779\n",
      "Epoch 1500: Loss = 0.2499993898466479\n",
      "Epoch 1600: Loss = 0.24999940851923538\n",
      "Epoch 1700: Loss = 0.24999942674876516\n",
      "Epoch 1800: Loss = 0.24999944454873932\n",
      "Epoch 1900: Loss = 0.2499994619321814\n",
      "Epoch 2000: Loss = 0.2499994789116549\n",
      "Epoch 2100: Loss = 0.2499994954992814\n",
      "Epoch 2200: Loss = 0.24999951170675858\n",
      "Epoch 2300: Loss = 0.24999952754537702\n",
      "Epoch 2400: Loss = 0.24999954302603677\n",
      "Epoch 2500: Loss = 0.24999955815926253\n",
      "Epoch 2600: Loss = 0.24999957295521907\n",
      "Epoch 2700: Loss = 0.2499995874237251\n",
      "Epoch 2800: Loss = 0.24999960157426715\n",
      "Epoch 2900: Loss = 0.24999961541601204\n",
      "Epoch 3000: Loss = 0.24999962895782002\n",
      "Epoch 3100: Loss = 0.24999964220825605\n",
      "Epoch 3200: Loss = 0.2499996551756013\n",
      "Epoch 3300: Loss = 0.2499996678678644\n",
      "Epoch 3400: Loss = 0.2499996802927913\n",
      "Epoch 3500: Loss = 0.24999969245787537\n",
      "Epoch 3600: Loss = 0.2499997043703673\n",
      "Epoch 3700: Loss = 0.24999971603728363\n",
      "Epoch 3800: Loss = 0.24999972746541582\n",
      "Epoch 3900: Loss = 0.24999973866133857\n",
      "Epoch 4000: Loss = 0.2499997496314179\n",
      "Epoch 4100: Loss = 0.24999976038181854\n",
      "Epoch 4200: Loss = 0.24999977091851172\n",
      "Epoch 4300: Loss = 0.24999978124728173\n",
      "Epoch 4400: Loss = 0.2499997913737332\n",
      "Epoch 4500: Loss = 0.24999980130329724\n",
      "Epoch 4600: Loss = 0.24999981104123764\n",
      "Epoch 4700: Loss = 0.24999982059265705\n",
      "Epoch 4800: Loss = 0.2499998299625026\n",
      "Epoch 4900: Loss = 0.24999983915557156\n",
      "Epoch 5000: Loss = 0.24999984817651622\n",
      "Epoch 5100: Loss = 0.24999985702984934\n",
      "Epoch 5200: Loss = 0.24999986571994887\n",
      "Epoch 5300: Loss = 0.24999987425106285\n",
      "Epoch 5400: Loss = 0.24999988262731326\n",
      "Epoch 5500: Loss = 0.2499998908527013\n",
      "Epoch 5600: Loss = 0.24999989893111058\n",
      "Epoch 5700: Loss = 0.24999990686631196\n",
      "Epoch 5800: Loss = 0.2499999146619667\n",
      "Epoch 5900: Loss = 0.24999992232163049\n",
      "Epoch 6000: Loss = 0.24999992984875702\n",
      "Epoch 6100: Loss = 0.2499999372467013\n",
      "Epoch 6200: Loss = 0.24999994451872304\n",
      "Epoch 6300: Loss = 0.2499999516679895\n",
      "Epoch 6400: Loss = 0.24999995869757896\n",
      "Epoch 6500: Loss = 0.24999996561048343\n",
      "Epoch 6600: Loss = 0.24999997240961164\n",
      "Epoch 6700: Loss = 0.2499999790977913\n",
      "Epoch 6800: Loss = 0.24999998567777243\n",
      "Epoch 6900: Loss = 0.249999992152229\n",
      "Epoch 7000: Loss = 0.24999999852376253\n",
      "Epoch 7100: Loss = 0.25000000479490303\n",
      "Epoch 7200: Loss = 0.2500000109681124\n",
      "Epoch 7300: Loss = 0.2500000170457862\n",
      "Epoch 7400: Loss = 0.2500000230302557\n",
      "Epoch 7500: Loss = 0.2500000289237897\n",
      "Epoch 7600: Loss = 0.2500000347285969\n",
      "Epoch 7700: Loss = 0.25000004044682766\n",
      "Epoch 7800: Loss = 0.2500000460805757\n",
      "Epoch 7900: Loss = 0.2500000516318799\n",
      "Epoch 8000: Loss = 0.250000057102726\n",
      "Epoch 8100: Loss = 0.2500000624950481\n",
      "Epoch 8200: Loss = 0.2500000678107307\n",
      "Epoch 8300: Loss = 0.2500000730516096\n",
      "Epoch 8400: Loss = 0.2500000782194737\n",
      "Epoch 8500: Loss = 0.2500000833160664\n",
      "Epoch 8600: Loss = 0.2500000883430871\n",
      "Epoch 8700: Loss = 0.2500000933021922\n",
      "Epoch 8800: Loss = 0.25000009819499647\n",
      "Epoch 8900: Loss = 0.25000010302307457\n",
      "Epoch 9000: Loss = 0.250000107787962\n",
      "Epoch 9100: Loss = 0.2500001124911563\n",
      "Epoch 9200: Loss = 0.250000117134118\n",
      "Epoch 9300: Loss = 0.250000121718272\n",
      "Epoch 9400: Loss = 0.25000012624500856\n",
      "Epoch 9500: Loss = 0.2500001307156841\n",
      "Epoch 9600: Loss = 0.25000013513162234\n",
      "Epoch 9700: Loss = 0.25000013949411526\n",
      "Epoch 9800: Loss = 0.25000014380442387\n",
      "Epoch 9900: Loss = 0.2500001480637793\n",
      "Predictions:\n",
      "[[0.49901995 0.50002656 0.49997583 0.50098113]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # XOR problem\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T \n",
    "    y = np.array([[0], [1], [1], [0]]).T \n",
    "\n",
    "    # Initialize the MLP with 2 input neurons, 2 hidden neurons, and 1 output neuron\n",
    "    mlp = MLP(layer_sizes=[2, 2, 1], activation_function='sigmoid')\n",
    "\n",
    "    mlp.train(X, y, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "    predictions = mlp.predict(X)\n",
    "    print(\"Predictions:\")\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ca388-5b81-492b-bc99-5199a6038fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
