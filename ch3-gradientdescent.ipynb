{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ba93c9-428f-4873-91d8-915544f2abf2",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f387fa-7b94-424a-a6fb-9f54cd38bcab",
   "metadata": {},
   "source": [
    "## 1. Cost Function\n",
    "For a **linear regression** problem, we define the cost function using **Mean Squared Error (MSE)**:\n",
    "\n",
    "$J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2$\n",
    "\n",
    "where:\n",
    "- $m$ is the number of training samples.\n",
    "- $h(x_i)$ is the predicted value:\n",
    "  $h(x) = w \\cdot x + b$\n",
    "- $y_i$ is the actual value.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Gradient Calculation\n",
    "To minimize $J(w, b)$, we compute the **partial derivatives** (gradients):\n",
    "\n",
    "### **Gradient of Weight \\( w \\):**\n",
    "$\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i) - y_i) \\cdot x_i$\n",
    "\n",
    "### **Gradient of Bias \\( b \\):**\n",
    "$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i) - y_i)$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Update Rule\n",
    "The parameters are updated using gradient descent:\n",
    "\n",
    "$w = w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}$\n",
    "$b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ (learning rate) controls the step size of the updates.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Algorithm Steps\n",
    "1. **Initialize** weights $w$ and bias $b$ to zero.\n",
    "2. **Compute** predictions using $h(x) = w \\cdot x + b$.\n",
    "3. **Calculate** the gradients $\\frac{\\partial J}{\\partial w}$ and $\\frac{\\partial J}{\\partial b}$.\n",
    "4. **Update** $w$ and $b$ using the gradient descent update rule.\n",
    "5. **Repeat** for a fixed number of epochs or until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Convergence\n",
    "- The learning rate $\\alpha$ must be **small enough** to ensure convergence.\n",
    "- If $\\alpha$ is **too large**, the algorithm may oscillate or diverge.\n",
    "- If $\\alpha$ is **too small**, the updates will be very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9977aafa-4bc4-4c12-9492-2ee944ee3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, lr=0.01, epochs=1000):\n",
    "    \"\"\"\n",
    "    Gradient descent to optimize weights for a simple linear regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    X: Feature matrix (n_samples, n_features)\n",
    "    y: Target vector (n_samples,)\n",
    "    lr: Learning rate \n",
    "    epochs: Number of iterations \n",
    "    \n",
    "    Returns:\n",
    "    w: Optimized weight vector\n",
    "    b: Optimized bias\n",
    "    cost_history: Cost function values over epochs\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)  # initialize weights\n",
    "    b = 0                     # bias\n",
    "    cost_history = []      \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # predictions\n",
    "        y_pred = np.dot(X, w) + b  \n",
    "\n",
    "        # gradients\n",
    "        dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))  # Gradient of w\n",
    "        db = (1 / n_samples) * np.sum(y_pred - y)         # Gradient of b\n",
    "\n",
    "        # update parameters\n",
    "        w -= lr * dw  \n",
    "        b -= lr * db  \n",
    "\n",
    "        # cost (Mean Squared Error)\n",
    "        cost = (1 / (2 * n_samples)) * np.sum((y_pred - y) ** 2)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print progress \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Cost = {cost}\")\n",
    "\n",
    "    return w, b, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aaa8771-2058-403e-9a1a-713bd2d9dd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cost = 22.0\n",
      "Epoch 100: Cost = 0.0007960877325255306\n",
      "Epoch 200: Cost = 2.630723062537505e-05\n",
      "Epoch 300: Cost = 8.693393389963599e-07\n",
      "Epoch 400: Cost = 2.8727877026972505e-08\n",
      "Epoch 500: Cost = 9.493311546510007e-10\n",
      "Epoch 600: Cost = 3.1371257972561346e-11\n",
      "Epoch 700: Cost = 1.036683376320491e-12\n",
      "Epoch 800: Cost = 3.4257868304825424e-14\n",
      "Epoch 900: Cost = 1.1320732798268987e-15\n",
      "Optimized Weights: [1.99999999]\n",
      "Optimized Bias: 2.027462363290322e-08\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X = np.array([[1], [2], [3], [4], [5]])  # Features\n",
    "    y = np.array([2, 4, 6, 8, 10])           # Target  \n",
    "\n",
    "    w, b, cost_history = gradient_descent(X, y, lr=0.1, epochs=1000)\n",
    "\n",
    "    print(f\"Optimized Weights: {w}\")\n",
    "    print(f\"Optimized Bias: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530092c9-4f07-4f99-9408-290219771a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
